{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "lim1 = 10000\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    \n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "        if i == lim1:\n",
    "            break\n",
    "    return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model consists of:\n",
    "\n",
    "- 0 for beauty\n",
    "- 1 for phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# for preprocessing\n",
    "\n",
    "#source\n",
    "df_beauty = getDF('reviews_Beauty_5.json.gz')\n",
    "df_phone = getDF('reviews_Cell_Phones_and_Accessories_5.json.gz')\n",
    "\n",
    "#target\n",
    "df_health = getDF('reviews_Health_and_Personal_Care_5.json.gz')\n",
    "df_apps = getDF('reviews_Apps_for_Android_5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       reviewerID        asin reviewerName helpful  \\\n",
      "0  A1YJEY40YUW4SE  7806397051       Andrea  [3, 4]   \n",
      "1   A60XNB876KYML  7806397051   Jessica H.  [1, 1]   \n",
      "2  A3G6XNM240RMWA  7806397051        Karen  [0, 1]   \n",
      "3  A1PQFP6SAJ6D80  7806397051        Norah  [2, 2]   \n",
      "4  A38FVHZTNQ271F  7806397051    Nova Amor  [0, 0]   \n",
      "\n",
      "                                          reviewText  overall  \\\n",
      "0  Very oily and creamy. Not at all what I expect...      1.0   \n",
      "1  This palette was a decent price and I was look...      3.0   \n",
      "2  The texture of this concealer pallet is fantas...      4.0   \n",
      "3  I really can't tell what exactly this thing is...      2.0   \n",
      "4  It was a little smaller than I expected, but t...      3.0   \n",
      "\n",
      "                  summary  unixReviewTime   reviewTime  \n",
      "0  Don't waste your money      1391040000  01 30, 2014  \n",
      "1             OK Palette!      1397779200  04 18, 2014  \n",
      "2           great quality      1378425600   09 6, 2013  \n",
      "3  Do not work on my face      1386460800   12 8, 2013  \n",
      "4              It's okay.      1382140800  10 19, 2013  \n"
     ]
    }
   ],
   "source": [
    "#target 2\n",
    "lim1 = 9000\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    \n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "        if i == lim1:\n",
    "            break\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df_tools = getDF('reviews_Tools_and_Home_Improvement_5.json.gz')\n",
    "df_food = getDF('reviews_Grocery_and_Gourmet_Food_5.json.gz')\n",
    "\n",
    "print(df_beauty.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# source domain\n",
    "\n",
    "# column 1\n",
    "c1_1 = df_beauty['reviewText']\n",
    "c1_2 = df_phone['reviewText']\n",
    "c1 = c1_1.append(c1_2, ignore_index=True)\n",
    "\n",
    "\n",
    "# column 2\n",
    "c2_1 = pd.Series(np.zeros(df_beauty.shape[0], dtype=int))\n",
    "c2_2 = pd.Series(np.ones(df_phone.shape[0], dtype=int))\n",
    "c2 = c2_1.append(c2_2, ignore_index=True)\n",
    "\n",
    "df = pd.DataFrame({'label': c2, 'features': c1})\n",
    "df.to_csv('rev_pre.csv', index=False)\n",
    "\n",
    "\n",
    "with open(\"rev_pre.csv\",'r') as f:\n",
    "    with open(\"rev_source.csv\",'w') as f1:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if i:\n",
    "                f1.write(line)\n",
    "            i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# target domain\n",
    "\n",
    "# column 1\n",
    "c1_1 = df_health['reviewText']\n",
    "c1_2 = df_apps['reviewText']\n",
    "c1 = c1_1.append(c1_2, ignore_index=True)\n",
    "\n",
    "\n",
    "# column 2\n",
    "c2_1 = pd.Series(np.zeros(df_health.shape[0], dtype=int))\n",
    "c2_2 = pd.Series(np.ones(df_apps.shape[0], dtype=int))\n",
    "c2 = c2_1.append(c2_2, ignore_index=True)\n",
    "\n",
    "df = pd.DataFrame({'label': c2, 'features': c1})\n",
    "df.to_csv('rev_pre.csv', index=False)\n",
    "\n",
    "\n",
    "with open(\"rev_pre.csv\",'r') as f:\n",
    "    with open(\"rev_target1.csv\",'w') as f1:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if i:\n",
    "                f1.write(line)\n",
    "            i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# target domain 2\n",
    "\n",
    "# column 1\n",
    "c1_1 = df_tools['reviewText']\n",
    "c1_2 = df_food['reviewText']\n",
    "c1 = c1_1.append(c1_2, ignore_index=True)\n",
    "\n",
    "\n",
    "# column 2\n",
    "c2_1 = pd.Series(np.zeros(df_tools.shape[0], dtype=int))\n",
    "c2_2 = pd.Series(np.ones(df_food.shape[0], dtype=int))\n",
    "c2 = c2_1.append(c2_2, ignore_index=True)\n",
    "\n",
    "df = pd.DataFrame({'label': c2, 'features': c1})\n",
    "df.to_csv('rev_pre.csv', index=False)\n",
    "\n",
    "\n",
    "with open(\"rev_pre.csv\",'r') as f:\n",
    "    with open(\"rev_target2.csv\",'w') as f1:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if i:\n",
    "                f1.write(line)\n",
    "            i = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "from torchtext import data\n",
    "\n",
    "assert torch.__version__ == '1.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'palette', 'was', 'a', 'decent', 'price', 'and', 'i', 'was', 'looking', 'for', 'a', 'few', 'different', 'shades.', 'this', 'palette', 'conceals', 'decently,', 'however,', 'it', 'does', 'somewhat', 'cake', 'up', 'and', 'crease.']\n",
      "0\n",
      "['good', 'style', 'is', 'essential', 'to', 'any', 'apple', 'design.', 'i', 'have', 'never', 'understood', 'why', 'third', 'party', 'vendors', 'have', 'offered', 'cases', 'for', 'the', 'iphone', '3g', 'and', '3gs', 'which', 'look', 'like', 'they', 'were', 'designed', 'by', 'an', 'ad', 'hoc', 'committee', 'at', 'the', 'lenin', 'tankworks', 'in', 'minsk.here', 'we', 'have', 'an', 'exception.', 'the', 'mophie', 'case', 'is', 'simple,', 'elegant', 'and', 'completely', 'complementary', 'to', \"apple's\", 'styling.', 'but', \"likewrigley's\", '5', 'stick', 'doublemint', 'gum', '40', 'packs,', '\"it\\'s', 'two,', 'two...\"', '-two', 'devices', 'in', 'one.', 'it', 'has', 'a', 'powerful', 'external', 'battery', 'that', 'extends', 'the', 'notoriously', 'battery', 'like', 'of', 'the', 'original', '3g', 'several', 'times.my', 'original', 'plan', 'was', 'to', 'use', 'this', 'hard', 'case', 'when', 'i', 'needed', 'extra', 'battery', 'life.', 'it', 'does', 'present', 'some', 'difficulty', 'in', 'that', 'the', 'apple', 'pin', 'is', 'covered', 'and', 'the', 'usb', 'connector', 'cannot', 'replicate', 'all', 'apple', 'connection', 'features.', 'but', 'then', 'again,', 'so', 'what?', 'the', 'essence', 'of', 'an', 'iphone', 'is', 'still', 'there.', 'i', 'am', 'using', 'it', 'full', 'time.']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# source\n",
    "\n",
    "FEATURES_FREE = data.Field(lower=True)\n",
    "LABEL_FREE = data.Field(sequential=False, is_target=True, unk_token=None)\n",
    "#FEATURES_FREE = data.Field(tokenize=lambda s: s.split(\" \"))\n",
    "\n",
    "rt_polarity_free = data.TabularDataset(\n",
    "    path=r'C:\\Users\\luiss\\Documents\\1ScriptsPython\\NLP\\rev_source.csv', format='csv',\n",
    "    fields=[('label', LABEL_FREE), ('features', FEATURES_FREE)])\n",
    "\n",
    "FEATURES_FREE.build_vocab(rt_polarity_free)\n",
    "LABEL_FREE.build_vocab(rt_polarity_free)\n",
    "\n",
    "\n",
    "# Show one example\n",
    "print(rt_polarity_free[1].features)\n",
    "print(rt_polarity_free[1].label)\n",
    "\n",
    "print(rt_polarity_free[-1].features)\n",
    "print(rt_polarity_free[-1].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86674 2\n"
     ]
    }
   ],
   "source": [
    "print(len(FEATURES_FREE.vocab), len(LABEL_FREE.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_INDEX = 1\n",
    "\n",
    "# Model\n",
    "class LogisticRegression(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Logistic Regression implementation based on torchtext input format.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.normal(torch.zeros(num_features)), requires_grad=True)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x: a batch of input text of shape [max_sentence_length, batch_size] using 1 for padding. \n",
    "        \"\"\"\n",
    "        \n",
    "        # retrieve weights and set those to zero that come from padding cells \n",
    "        active_tokens_mask = (x != PAD_INDEX).float()\n",
    "        #print(active_tokens_mask.shape)\n",
    "        #print(x.shape)\n",
    "        #print(self.weights.shape)\n",
    "        filtered = active_tokens_mask * self.weights[x]\n",
    "        \n",
    "        # sum pooling along the token position dimension \n",
    "        logits = filtered.sum(0)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for the training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def accuracy(dataset, model, batch_size=32):\n",
    "    # Testing the model and returning the accuracy on the given dataset\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch in data.BucketIterator(dataset=dataset, batch_size=batch_size):\n",
    "        output = model(batch.features)\n",
    "        total += len(batch.label)\n",
    "        prediction = (output > 0).long()\n",
    "        correct += (prediction == batch.label).sum()\n",
    "\n",
    "    return float(correct) / total  \n",
    "\n",
    "def training_loop(model, train_set, dev_set, num_epochs=100, \n",
    "                  batch_size=32, lr=0.1, weight_decay=0.0, print_val=100):\n",
    "    \"\"\"\n",
    "    Should return the best dev_set accuracy and the number of epochs used. \n",
    "    \"\"\"\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()  \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, \n",
    "                              weight_decay=weight_decay)  \n",
    "    # Training the Model\n",
    "    epoch_accuracies = []\n",
    "    epoch_acc_list = []\n",
    "    best_epoch = 0\n",
    "    best_accuracy = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, batch in enumerate(data.BucketIterator(dataset=train_set, batch_size=batch_size)):\n",
    "            features = batch.features\n",
    "            labels = batch.label.float()\n",
    "            #print(features)\n",
    "            #print(labels)\n",
    "\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % print_val == 0:\n",
    "                print ('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f, Dev: %.4f' \n",
    "                     % (epoch+1, num_epochs, i+1, len(train_set)//batch_size, \n",
    "                        loss.data, accuracy(dev_set, model)))\n",
    "\n",
    "        epoch_accuracies.append(accuracy(dev_set, model))\n",
    "        epoch_acc_list.append(accuracy(dev_set, model))\n",
    "        if epoch_accuracies[-1] > best_accuracy:\n",
    "            best_accuracy = epoch_accuracies[-1]\n",
    "            best_epoch = epoch\n",
    "\n",
    "        # TODO: early stopping here  \n",
    "        if epoch>10:\n",
    "            if epoch_accuracies[-11] >= epoch_accuracies[-1]:\n",
    "                break\n",
    "    \n",
    "    return epoch_acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14000\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "# initialise the data\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "rt_train_free, rt_dev_free = rt_polarity_free.split([0.7, 0.3], random_state=random.getstate())\n",
    "\n",
    "# TODO: implement grid search to make sure the above 4 variable have the right values \n",
    "train_set = rt_train_free\n",
    "dev_set = rt_dev_free\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(dev_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \t lr:  1.0 weigth_decay:  0.0001\n",
      "Epoch: [1/10], Step: [200/437], Loss: 0.0867, Dev: 0.8693\n",
      "Epoch: [1/10], Step: [400/437], Loss: 0.0219, Dev: 0.9018\n",
      "Epoch: [2/10], Step: [200/437], Loss: 0.0145, Dev: 0.9137\n",
      "Epoch: [2/10], Step: [400/437], Loss: 0.0231, Dev: 0.9210\n",
      "Epoch: [3/10], Step: [200/437], Loss: 0.0112, Dev: 0.9250\n",
      "Epoch: [3/10], Step: [400/437], Loss: 0.0163, Dev: 0.9328\n",
      "Epoch: [4/10], Step: [200/437], Loss: 0.0126, Dev: 0.9332\n",
      "Epoch: [4/10], Step: [400/437], Loss: 0.0160, Dev: 0.9390\n",
      "Epoch: [5/10], Step: [200/437], Loss: 0.0150, Dev: 0.9377\n",
      "Epoch: [5/10], Step: [400/437], Loss: 0.0152, Dev: 0.9415\n",
      "Epoch: [6/10], Step: [200/437], Loss: 0.0174, Dev: 0.9402\n",
      "Epoch: [6/10], Step: [400/437], Loss: 0.0128, Dev: 0.9437\n",
      "Epoch: [7/10], Step: [200/437], Loss: 0.0186, Dev: 0.9427\n",
      "Epoch: [7/10], Step: [400/437], Loss: 0.0113, Dev: 0.9452\n",
      "Epoch: [8/10], Step: [200/437], Loss: 0.0191, Dev: 0.9443\n",
      "Epoch: [8/10], Step: [400/437], Loss: 0.0108, Dev: 0.9475\n",
      "Epoch: [9/10], Step: [200/437], Loss: 0.0195, Dev: 0.9458\n",
      "Epoch: [9/10], Step: [400/437], Loss: 0.0108, Dev: 0.9485\n",
      "Epoch: [10/10], Step: [200/437], Loss: 0.0198, Dev: 0.9468\n",
      "Epoch: [10/10], Step: [400/437], Loss: 0.0111, Dev: 0.9488\n",
      "[0.8973333333333333, 0.9216666666666666, 0.9316666666666666, 0.9365, 0.9411666666666667, 0.9431666666666667, 0.945, 0.9451666666666667, 0.946, 0.9481666666666667]\n"
     ]
    }
   ],
   "source": [
    "ilr = 1.0\n",
    "iw = 0.0001\n",
    "\n",
    "print('\\n \\t lr: ',ilr, 'weigth_decay: ',iw)\n",
    "\n",
    "model_source = LogisticRegression(len(FEATURES_FREE.vocab))\n",
    "\n",
    "epoch_list = training_loop(model_source, train_set, dev_set, \n",
    "                                       num_epochs=10, batch_size=32,\n",
    "                                       lr=ilr, weight_decay=iw, print_val = 200)\n",
    "print(epoch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer 1 - similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'would', 'recommend', 'this', 'for', 'a', 'travel', 'magnifier', 'for', 'the', 'occasional', 'reading.i', 'had', 'read', 'on', 'another', 'review', 'about', 'a', 'magnifier', 'having', 'a', 'problem', 'with', 'the', 'light', 'coming', 'on.', 'i', 'did', 'find', 'that', 'this', 'one', 'appeared', 'to', 'be', 'doa', 'out', 'of', 'the', 'box.', 'but,', 'after', 'opening', '&', 'shutting', 'the', 'viewer', 'to', 'turn', 'on', '&', 'off', 'the', 'light,', 'the', 'light', 'began', 'to', 'come', 'on.', 'after', 'several', 'times', 'of', 'doing', 'this,', 'the', 'light', 'appears', 'to', 'be', 'coming', 'on', 'all', 'the', 'time.it', 'is', 'small,', 'but', 'for', 'taking', 'it', 'someplace', '&', 'reading', 'things', 'like', 'a', 'menu', 'in', 'a', 'dark', 'corner', 'of', 'a', 'restaurant,', 'this', 'is', 'great.']\n",
      "0\n",
      "['this', 'is', 'a', 'great', 'app.', \"i'm\", 'doing', 'the', 'leangains', 'diet', 'and', 'this', 'app', 'makes', 'calorie', 'counting', 'very', 'simple', 'and', 'accessible.', \"i'm\", 'constantly', 'surprised', 'by', 'how', 'many', 'foods', 'with', 'the', 'correct', 'nutritional', 'info', 'it', 'has', 'in', 'its', 'database.']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# target\n",
    "\n",
    "FEATURES_TAR = data.Field(lower=True)\n",
    "LABEL_TAR = data.Field(sequential=False, is_target=True, unk_token=None)\n",
    "\n",
    "rt_polarity_tar = data.TabularDataset(\n",
    "    path=r'C:\\Users\\luiss\\Documents\\1ScriptsPython\\NLP\\rev_target1.csv', format='csv',\n",
    "    fields=[('label', LABEL_TAR), ('features', FEATURES_TAR)])\n",
    "\n",
    "FEATURES_TAR.build_vocab(rt_polarity_tar)\n",
    "LABEL_TAR.build_vocab(rt_polarity_tar)\n",
    "\n",
    "\n",
    "# Show one example\n",
    "print(rt_polarity_tar[1].features)\n",
    "print(rt_polarity_tar[1].label)\n",
    "\n",
    "print(rt_polarity_tar[-1].features)\n",
    "print(rt_polarity_tar[-1].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69383 2\n"
     ]
    }
   ],
   "source": [
    "print(len(FEATURES_TAR.vocab), len(LABEL_TAR.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer 1 - run model in big set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14000\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "# initialise the data\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "rt_train_tar, rt_dev_tar = rt_polarity_tar.split([0.7, 0.3], random_state=random.getstate())\n",
    "\n",
    "# TODO: implement grid search to make sure the above 4 variable have the right values \n",
    "train_set_big = rt_train_tar\n",
    "dev_set_big = rt_dev_tar\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(dev_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \t lr:  1.0 weigth_decay:  0.0001\n",
      "Epoch: [1/10], Step: [200/437], Loss: 0.1998, Dev: 0.8658\n",
      "Epoch: [1/10], Step: [400/437], Loss: 0.4183, Dev: 0.9032\n",
      "Epoch: [2/10], Step: [200/437], Loss: 0.1792, Dev: 0.9138\n",
      "Epoch: [2/10], Step: [400/437], Loss: 0.3085, Dev: 0.9265\n",
      "Epoch: [3/10], Step: [200/437], Loss: 0.1406, Dev: 0.9285\n",
      "Epoch: [3/10], Step: [400/437], Loss: 0.2474, Dev: 0.9343\n",
      "Epoch: [4/10], Step: [200/437], Loss: 0.1177, Dev: 0.9350\n",
      "Epoch: [4/10], Step: [400/437], Loss: 0.1633, Dev: 0.9417\n",
      "Epoch: [5/10], Step: [200/437], Loss: 0.1038, Dev: 0.9413\n",
      "Epoch: [5/10], Step: [400/437], Loss: 0.1387, Dev: 0.9465\n",
      "Epoch: [6/10], Step: [200/437], Loss: 0.0916, Dev: 0.9463\n",
      "Epoch: [6/10], Step: [400/437], Loss: 0.1157, Dev: 0.9507\n",
      "Epoch: [7/10], Step: [200/437], Loss: 0.0823, Dev: 0.9487\n",
      "Epoch: [7/10], Step: [400/437], Loss: 0.0979, Dev: 0.9535\n",
      "Epoch: [8/10], Step: [200/437], Loss: 0.0751, Dev: 0.9520\n",
      "Epoch: [8/10], Step: [400/437], Loss: 0.0870, Dev: 0.9543\n",
      "Epoch: [9/10], Step: [200/437], Loss: 0.0694, Dev: 0.9535\n",
      "Epoch: [9/10], Step: [400/437], Loss: 0.0799, Dev: 0.9568\n",
      "Epoch: [10/10], Step: [200/437], Loss: 0.0649, Dev: 0.9563\n",
      "Epoch: [10/10], Step: [400/437], Loss: 0.0746, Dev: 0.9580\n",
      "[0.8746666666666667, 0.9278333333333333, 0.9373333333333334, 0.9425, 0.945, 0.949, 0.9523333333333334, 0.9535, 0.955, 0.958]\n"
     ]
    }
   ],
   "source": [
    "ilr = 1.0\n",
    "iw = 0.0001\n",
    "\n",
    "print('\\n \\t lr: ',ilr, 'weigth_decay: ',iw)\n",
    "\n",
    "model_tar = model_source\n",
    "\n",
    "epoch_list = training_loop(model_tar, train_set_big, dev_set_big, \n",
    "                           num_epochs=10, batch_size=32,\n",
    "                           lr=ilr, weight_decay=iw, print_val = 200)\n",
    "print(epoch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer 1 - run model in small set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "19000\n"
     ]
    }
   ],
   "source": [
    "# initialise the data\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "#per_training_data = 0.1\n",
    "#spliting_list = [1 - per_training_data, 0.7 * per_training_data, 0.3 * per_training_data]\n",
    "#rt_surplus, rt_dev_tar, rt_train_tar = rt_polarity_tar.split(spliting_list, random_state=random.getstate())\n",
    "\n",
    "per_training_data = 0.05\n",
    "spliting_list = [per_training_data, 1 - per_training_data]\n",
    "rt_train_tar, rt_dev_tar = rt_polarity_tar.split(spliting_list, random_state=random.getstate())\n",
    "\n",
    "\n",
    "# TODO: implement grid search to make sure the above 4 variable have the right values \n",
    "train_set_small = rt_train_tar\n",
    "dev_set_small = rt_dev_tar\n",
    "\n",
    "print(len(train_set_small))\n",
    "print(len(dev_set_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \t lr:  1.0 weigth_decay:  0.0001\n",
      "Epoch: [1/10], Step: [10/12], Loss: 2.4424, Dev: 0.5995\n",
      "Epoch: [2/10], Step: [10/12], Loss: 1.8609, Dev: 0.6513\n",
      "Epoch: [3/10], Step: [10/12], Loss: 1.4075, Dev: 0.6962\n",
      "Epoch: [4/10], Step: [10/12], Loss: 1.0821, Dev: 0.7238\n",
      "Epoch: [5/10], Step: [10/12], Loss: 0.8715, Dev: 0.7427\n",
      "Epoch: [6/10], Step: [10/12], Loss: 0.7475, Dev: 0.7575\n",
      "Epoch: [7/10], Step: [10/12], Loss: 0.6543, Dev: 0.7661\n",
      "Epoch: [8/10], Step: [10/12], Loss: 0.5696, Dev: 0.7732\n",
      "Epoch: [9/10], Step: [10/12], Loss: 0.5010, Dev: 0.7800\n",
      "Epoch: [10/10], Step: [10/12], Loss: 0.4414, Dev: 0.7846\n",
      "[0.6243684210526316, 0.6714736842105263, 0.7021578947368421, 0.7303157894736843, 0.748578947368421, 0.7519473684210526, 0.7598421052631579, 0.7692105263157895, 0.7757368421052632, 0.7832105263157895]\n"
     ]
    }
   ],
   "source": [
    "ilr = 1.0\n",
    "iw = 0.0001\n",
    "\n",
    "print('\\n \\t lr: ',ilr, 'weigth_decay: ',iw)\n",
    "\n",
    "model_tar2 = model_source\n",
    "\n",
    "epoch_list = training_loop(model_tar2, train_set_small, dev_set_small, \n",
    "                           num_epochs=10, batch_size=80,\n",
    "                           lr=ilr, weight_decay=iw, print_val = 10)\n",
    "print(epoch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer 1 - run model in small set 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# initialise the data\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "#per_training_data = 0.1\n",
    "#spliting_list = [1 - per_training_data, 0.7 * per_training_data, 0.3 * per_training_data]\n",
    "#rt_surplus, rt_dev_tar, rt_train_tar = rt_polarity_tar.split(spliting_list, random_state=random.getstate())\n",
    "\n",
    "per_training_data = 0.5\n",
    "spliting_list = [per_training_data, 1 - per_training_data]\n",
    "rt_train_tar, rt_dev_tar = rt_polarity_tar.split(spliting_list, random_state=random.getstate())\n",
    "\n",
    "\n",
    "# TODO: implement grid search to make sure the above 4 variable have the right values \n",
    "train_set_small = rt_train_tar\n",
    "dev_set_small = rt_dev_tar\n",
    "\n",
    "print(len(train_set_small))\n",
    "print(len(dev_set_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \t lr:  1.0 weigth_decay:  0.0001\n",
      "[0.8333, 0.8771, 0.8983, 0.9073, 0.9134, 0.9186, 0.9229, 0.9269, 0.9298, 0.9342]\n"
     ]
    }
   ],
   "source": [
    "ilr = 1.0\n",
    "iw = 0.0001\n",
    "\n",
    "print('\\n \\t lr: ',ilr, 'weigth_decay: ',iw)\n",
    "\n",
    "model_tar2 = model_source\n",
    "\n",
    "epoch_list = training_loop(model_tar2, train_set_small, dev_set_small, \n",
    "                           num_epochs=10, batch_size=80,\n",
    "                           lr=ilr, weight_decay=iw, print_val = 100)\n",
    "print(epoch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target 2 - different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84158\n",
      "['these', 'little', 'magnets', 'are', 'really', 'powerful', 'for', 'there', 'size.', 'i', 'am', 'using', 'them', 'to', 'make', 'secret', 'compartments', 'in', 'custom', 'made', 'boxes.', 'each', 'one', 'hols', 'about', '.8', 'of', 'a', 'pound.']\n",
      "0\n",
      "['this', 'is', 'very', 'good........i', 'add', 'a', 'little', 'ground', 'beef', 'sometimes.................good', 'either', 'way.very', 'easy', 'and', 'fast', 'to', 'prepare.for', 'some', 'reason,', 'it', 'is', 'hard', 'to', 'find', 'here;', 'so,', 'i', 'love', 'i', 'have', 'several', 'in', 'my', 'pantry.']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# target 2\n",
    "\n",
    "FEATURES_TAR2 = data.Field(lower=True)\n",
    "LABEL_TAR2 = data.Field(sequential=False, is_target=True, unk_token=None)\n",
    "\n",
    "rt_polarity_tar2 = data.TabularDataset(\n",
    "    path=r'C:\\Users\\luiss\\Documents\\1ScriptsPython\\NLP\\rev_target2.csv', format='csv',\n",
    "    fields=[('label', LABEL_TAR2), ('features', FEATURES_TAR2)])\n",
    "\n",
    "FEATURES_TAR2.build_vocab(rt_polarity_tar2)\n",
    "LABEL_TAR2.build_vocab(rt_polarity_tar2)\n",
    "\n",
    "print(len(FEATURES_TAR2.vocab))\n",
    "\n",
    "# Show one example\n",
    "print(rt_polarity_tar2[1].features)\n",
    "print(rt_polarity_tar2[1].label)\n",
    "\n",
    "print(rt_polarity_tar2[-1].features)\n",
    "print(rt_polarity_tar2[-1].label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer 2 - run model in big set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12600\n",
      "5400\n"
     ]
    }
   ],
   "source": [
    "# initialise the data\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "rt_train_tar2, rt_dev_tar2 = rt_polarity_tar2.split([0.7, 0.3], random_state=random.getstate())\n",
    "\n",
    "# TODO: implement grid search to make sure the above 4 variable have the right values \n",
    "train_set_big = rt_train_tar2\n",
    "dev_set_big = rt_dev_tar2\n",
    "\n",
    "print(len(train_set_big))\n",
    "print(len(dev_set_big))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \t lr:  1.0 weigth_decay:  0.0001\n",
      "Epoch: [1/10], Step: [100/157], Loss: 0.5837, Dev: 0.8256\n",
      "Epoch: [2/10], Step: [100/157], Loss: 0.2602, Dev: 0.8907\n",
      "Epoch: [3/10], Step: [100/157], Loss: 0.1646, Dev: 0.9120\n",
      "Epoch: [4/10], Step: [100/157], Loss: 0.1230, Dev: 0.9224\n",
      "Epoch: [5/10], Step: [100/157], Loss: 0.1014, Dev: 0.9300\n",
      "Epoch: [6/10], Step: [100/157], Loss: 0.0916, Dev: 0.9348\n",
      "Epoch: [7/10], Step: [100/157], Loss: 0.0812, Dev: 0.9393\n",
      "Epoch: [8/10], Step: [100/157], Loss: 0.0745, Dev: 0.9422\n",
      "Epoch: [9/10], Step: [100/157], Loss: 0.0688, Dev: 0.9446\n",
      "Epoch: [10/10], Step: [100/157], Loss: 0.0639, Dev: 0.9461\n",
      "[0.8657407407407407, 0.9016666666666666, 0.9162962962962963, 0.9268518518518518, 0.932962962962963, 0.9387037037037037, 0.9414814814814815, 0.9438888888888889, 0.9468518518518518, 0.9496296296296296]\n"
     ]
    }
   ],
   "source": [
    "ilr = 1.0\n",
    "iw = 0.0001\n",
    "\n",
    "print('\\n \\t lr: ',ilr, 'weigth_decay: ',iw)\n",
    "\n",
    "model_tar2 = model_source\n",
    "\n",
    "epoch_list = training_loop(model_tar2, train_set_big, dev_set_big, \n",
    "                           num_epochs=10, batch_size=80,\n",
    "                           lr=ilr, weight_decay=iw, print_val = 100)\n",
    "print(epoch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer 2 - run model in small set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "17100\n"
     ]
    }
   ],
   "source": [
    "# initialise the data\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "#per_training_data = 0.1\n",
    "#spliting_list = [1 - per_training_data, 0.7 * per_training_data, 0.3 * per_training_data]\n",
    "#rt_surplus, rt_dev_tar, rt_train_tar = rt_polarity_tar.split(spliting_list, random_state=random.getstate())\n",
    "\n",
    "per_training_data = 0.05\n",
    "spliting_list = [per_training_data, 1 - per_training_data]\n",
    "rt_train_tar2, rt_dev_tar2 = rt_polarity_tar2.split(spliting_list, random_state=random.getstate())\n",
    "\n",
    "\n",
    "# TODO: implement grid search to make sure the above 4 variable have the right values \n",
    "train_set_small = rt_train_tar2\n",
    "dev_set_small = rt_dev_tar2\n",
    "\n",
    "print(len(train_set_small))\n",
    "print(len(dev_set_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \t lr:  1.0 weigth_decay:  0.0001\n",
      "Epoch: [1/10], Step: [10/11], Loss: 2.2825, Dev: 0.6104\n",
      "Epoch: [2/10], Step: [10/11], Loss: 5.1326, Dev: 0.6694\n",
      "Epoch: [3/10], Step: [10/11], Loss: 2.3636, Dev: 0.7104\n",
      "Epoch: [4/10], Step: [10/11], Loss: 0.7029, Dev: 0.7373\n",
      "Epoch: [5/10], Step: [10/11], Loss: 0.5786, Dev: 0.7555\n",
      "Epoch: [6/10], Step: [10/11], Loss: 0.5849, Dev: 0.7325\n",
      "Epoch: [7/10], Step: [10/11], Loss: 0.4156, Dev: 0.7825\n",
      "Epoch: [8/10], Step: [10/11], Loss: 0.3261, Dev: 0.7879\n",
      "Epoch: [9/10], Step: [10/11], Loss: 0.2761, Dev: 0.7916\n",
      "Epoch: [10/10], Step: [10/11], Loss: 0.2545, Dev: 0.7968\n",
      "[0.5593567251461988, 0.5385380116959064, 0.6192397660818714, 0.7287719298245614, 0.7575438596491229, 0.5522807017543859, 0.7821637426900585, 0.7871929824561403, 0.79046783625731, 0.7947953216374269]\n"
     ]
    }
   ],
   "source": [
    "ilr = 1.0\n",
    "iw = 0.0001\n",
    "\n",
    "print('\\n \\t lr: ',ilr, 'weigth_decay: ',iw)\n",
    "\n",
    "model_tar2 = model_source\n",
    "\n",
    "epoch_list = training_loop(model_tar2, train_set_small, dev_set_small, \n",
    "                           num_epochs=10, batch_size=80,\n",
    "                           lr=ilr, weight_decay=iw, print_val = 10)\n",
    "print(epoch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "644px",
    "left": "1282.25px",
    "right": "20px",
    "top": "111.992px",
    "width": "274px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
